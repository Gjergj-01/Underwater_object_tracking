{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import cv2\n",
    "import cvzone\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.utils.data as data \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "## Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Yolo\n",
    "from ultralytics import YOLO\n",
    "\n",
    "import scipy.linalg\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparazione dei dati\n",
    "Il dataset originario era composto da 26067 immagini, mentre per motivi logistici in questo notebook è stato ridotto a 9014 immagini e 11 classi (invece delle 32 originarie). Per carciare le immagini usiamo la funzionalità `ImageLoader` presente in Pytorch che permette di estrarre tutte le immagini presenti in una cartella (e nelle sue sottocartelle) e assegna come lable a tali immagini il nome della cartella stessa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../archive\"\n",
    "\n",
    "# Facciamo override della classe originaria perchè vogliamo estrarre\n",
    "# anche l'indice dell'immagine, questo servirà dopo per la preparazione\n",
    "# del dataset nel formato richiesto per l'addestramento di Yolov8\n",
    "# (nello specifico l'indice ci serve per associare ad ogni immagine il suo bounding box)\n",
    "\n",
    "class ImageFolderWithIndices(datasets.ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        image, label = super().__getitem__(index)\n",
    "\n",
    "        path = self.imgs[index][0]\n",
    "        filename = os.path.basename(path)\n",
    "\n",
    "        str_num = ''\n",
    "        for elem in filename:\n",
    "            if elem.isdigit():\n",
    "                str_num += elem\n",
    "\n",
    "        num = (int) (str_num)\n",
    "    \n",
    "        return image, label, num\n",
    "    \n",
    "dataset = ImageFolderWithIndices(root=dataset_path)\n",
    "\n",
    "'''\n",
    "    Il dataset viene suddiviso nel modo seguente:\n",
    "        - 80% training_set\n",
    "        - 10% validation_set\n",
    "        - 10% test_set\n",
    "'''\n",
    "\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(dataset, [7212, 901, 901])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estrazione dei bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Il bounding box ritornato da questo script avrà la seguente forma:\n",
    "    bbox = [x_top_left, y_top_left, width, height, index], \n",
    "    dove index è l'indice dell'immagine nella cartella in cui è salvata,\n",
    "    questo serve per ricreare la corrispondenza fra immagine e lable\n",
    "'''\n",
    "\n",
    "bounding_boxes = []\n",
    "\n",
    "entries = sorted(os.listdir(dataset_path))\n",
    "for folder in entries:\n",
    "    sub_directory = os.path.join(dataset_path, folder)\n",
    "    tensor_list = []\n",
    "\n",
    "    for filename in os.listdir(sub_directory):\n",
    "        if filename.endswith(\"groundtruth_rect.txt\"):\n",
    "            path = os.path.join(sub_directory, filename)\n",
    "            f = open(path, \"r\")\n",
    "            lines = f.readlines()\n",
    "            len_file = len(lines)\n",
    "\n",
    "            for i in range(len_file):\n",
    "                line = lines[i].split()\n",
    "                floates = [float(x) for x in line]\n",
    "                index = (float)(i)\n",
    "                floates.append(index)\n",
    "                coordinates = torch.tensor(floates)\n",
    "                tensor_list.append(coordinates)\n",
    "        \n",
    "    bounding_boxes.append(tensor_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparazione del dataset per l'addestramento di YOLOv8 e ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"datasets/dataset\"\n",
    "resnet_dir = \"resnet_dir\"\n",
    "\n",
    "# creazione delle directory per Yolov8\n",
    "\n",
    "image_dirs = {\n",
    "    \"train\" : os.path.join(dataset_dir, \"images/train\"),\n",
    "    \"val\": os.path.join(dataset_dir, \"images/val\"),\n",
    "    \"test\": os.path.join(dataset_dir, \"images/test\")\n",
    "}\n",
    "\n",
    "label_dirs = {\n",
    "    \"train\": os.path.join(dataset_dir, \"labels/train\"),\n",
    "    \"val\": os.path.join(dataset_dir, \"labels/val\"),\n",
    "    \"test\": os.path.join(dataset_dir, \"labels/test\")\n",
    "}\n",
    "\n",
    "# Creazione delle directory per ResNet18\n",
    "\n",
    "dirs = {\n",
    "    \"train\" : os.path.join(resnet_dir, \"train\"),\n",
    "    \"val\" : os.path.join(resnet_dir, \"val\"),\n",
    "    \"test\" : os.path.join(resnet_dir, \"test\")\n",
    "}\n",
    "\n",
    "# Creazione delle cartelle\n",
    "\n",
    "for dir_path in image_dirs.values():\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "for dir_path in label_dirs.values():\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "for dir_path in dirs.values():\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# Salvataggio delle delle immagini e dei bounding boxes\n",
    "\n",
    "def save_images_and_labels(data, dataset_type):\n",
    "    image_dir = image_dirs[dataset_type]\n",
    "    label_dir = label_dirs[dataset_type]\n",
    "    cropped_image_dir = dirs[dataset_type]\n",
    "\n",
    "    for image, label, index in data:\n",
    "        img_save_path = os.path.join(image_dir, f\"{index}_{label}.jpg\")\n",
    "        image.save(img_save_path)\n",
    "\n",
    "        width = image.width\n",
    "        height = image.height\n",
    "\n",
    "        bounding_box = bounding_boxes[label][index-1]\n",
    "        bbox_width = bounding_box[2]\n",
    "        bbox_height = bounding_box[3]\n",
    "        \n",
    "        class_id = label\n",
    "\n",
    "        label_save_path = os.path.join(label_dir, f\"{index}_{label}.txt\")\n",
    "        with open(label_save_path, 'w') as f:\n",
    "\n",
    "            # normalizziamo il bbox\n",
    "\n",
    "            x_center = (bounding_box[0] + bbox_width/2) / width\n",
    "            y_center = (bounding_box[1] + bbox_height/2) / height\n",
    "            b_width = bbox_width / width\n",
    "            b_height = bbox_height / height\n",
    "\n",
    "            f.write(f\"{class_id} {x_center} {y_center} {b_width} {b_height}\\n\")\n",
    "\n",
    "        # Ritagliamo le immagini tenendoci solo la parte individuata dai bounding boxes\n",
    "\n",
    "        bbox_width = float(bbox_width)\n",
    "        bbox_height = float(bbox_height)\n",
    "        \n",
    "        x_min, y_min = round(float(bounding_box[0]), 1), round(float(bounding_box[1]), 1)\n",
    "        x_max = round(float(x_min + bbox_width), 1)\n",
    "        y_max = round(float(y_min + bbox_height), 1)\n",
    "\n",
    "        # Check che verifica che non si esca dai bordi dell'immagine o che non ci siano bounding boxes con \n",
    "        # altezza o spessore nullo\n",
    "        if x_min < 0 or y_min < 0 or x_max >= width or y_max >= height or x_max - x_min <= 0 or y_max - y_min <= 0:\n",
    "            continue\n",
    "        \n",
    "        cropped_image = image.crop((x_min, y_min, x_max, y_max))\n",
    "        cropped_save_path = os.path.join(cropped_image_dir, f\"{index}_{label}.jpg\")\n",
    "        cropped_image.save(cropped_save_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_images_and_labels(train_set, \"train\")\n",
    "save_images_and_labels(test_set, \"test\")\n",
    "save_images_and_labels(val_set, \"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yolov8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"best.pt\")     # cambiare con 'yolov8n.pt' in fase di addestramento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addestramento del modello\n",
    "result = model.train(data=\"file.yaml\", epochs=15, batch=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Risultati\n",
    "\n",
    "<img src=\"last_run/P_curve.png\" alt=\"P_curve\" width=\"500\" height=\"400\"/> <img src=\"last_run/PR_curve.png\" alt=\"PR_curve\" width=\"500\" height=\"400\"/> <img src=\"last_run/confusion_matrix.png\" alt=\"confusion_matrix\" width=\"500\" height=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracking con Yolov8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = model.track(source=\"Tracking_test/Ballena/Ballena.mp4\", conf=0.5, iou=0.6, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = model.track(source=\"Tracking_test/SeaDiver/SeaDiver.mp4\", conf=0.3, iou=0.5, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res3 = model.track(source=\"Tracking_test/Octopus2/Octopus2.mp4\", conf=0.5, iou=0.6, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Addestramento di ResNet18 per l'estrazione delle feature dai bounding boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nelle prime due celle di codice estraiamo le immagini al fine di calcolare la loro media e deviazione standard per poterle normalizzare in modo che complessivamente la media e la deviazione standard su tutto il dataset siano approssimativamente 0 e 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "tmp_dataset = datasets.ImageFolder(root=resnet_dir, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([0.2522, 0.4443, 0.4805])\n",
      "std: tensor([0.1188, 0.1406, 0.1375])\n"
     ]
    }
   ],
   "source": [
    "# Andiamo a calcolare la media e la deviazione standard per \n",
    "# poter poi normalizzare le immagini\n",
    "\n",
    "dataset_size = len(tmp_dataset)\n",
    "\n",
    "DATA_MEAN = 0.0\n",
    "var = 0.0\n",
    "\n",
    "for i in range(dataset_size):\n",
    "    image, _ = tmp_dataset[i]\n",
    "    DATA_MEAN += image.mean(dim=(1, 2))\n",
    "    var += image.var(dim=(1, 2))\n",
    "    \n",
    "\n",
    "DATA_MEAN /= dataset_size\n",
    "DATA_STD = torch.sqrt(var/dataset_size)\n",
    "\n",
    "print(f\"Mean: {DATA_MEAN}\")\n",
    "print(f\"std: {DATA_STD}\")\n",
    "\n",
    "\"\"\" \n",
    "Mean: tensor([0.2522, 0.4443, 0.4805])\n",
    "std: tensor([0.1188, 0.1406, 0.1375])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.2522, 0.4443, 0.4805], [0.1188, 0.1406, 0.1375])\n",
    "])\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.2522, 0.4443, 0.4805], [0.1188, 0.1406, 0.1375])\n",
    "])\n",
    "\n",
    "train_dataset_path = \"resnet_dir/train1/train\"\n",
    "val_dataset_path = \"resnet_dir/val1/val\"\n",
    "test_dataset_path = \"resnet_dir/test1/test\"\n",
    "\n",
    "class CostumImageDataset(Dataset):\n",
    "    def __init__(self, image_folder, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.image_filenames = os.listdir(image_folder)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_folder, self.image_filenames[idx])\n",
    "        image = Image.open(img_path)\n",
    "        filename = self.image_filenames[idx].split('_')\n",
    "        category = filename[1].split('.')\n",
    "        label = int(category[0])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return (image, label)\n",
    "    \n",
    "\n",
    "train_dataset = CostumImageDataset(train_dataset_path, train_transform)\n",
    "val_dataset = CostumImageDataset(val_dataset_path, test_transform)\n",
    "test_dataset = CostumImageDataset(val_dataset_path, test_transform)\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch mean tensor([-0.1059, -0.0629,  0.0056])\n",
      "Batch std tensor([1.3584, 1.3395, 1.6535])\n"
     ]
    }
   ],
   "source": [
    "imgs, _ = next(iter(train_loader))\n",
    "print(\"Batch mean\", imgs.mean(dim=[0,2,3]))\n",
    "print(\"Batch std\", imgs.std(dim=[0,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_resnet18(model, optimizer, data_loader, loss_module, num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    # Parallelize training accross multiple GPUs\n",
    "    # model = torch.nn.DataParallel(model)\n",
    "\n",
    "    loss = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, labels in data_loader:\n",
    "            # Questo passaggio è strettamente necessrio solo se si usa una gpu\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            preds = model(inputs)\n",
    "            preds = preds.squeeze(dim=1)\n",
    "\n",
    "            loss = loss_module(preds, labels)\n",
    "\n",
    "            # Prima di calcolare i gradienti ci assicuriamo che siano tutti zero\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Aggiornamento dei parametri\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model = models.resnet18(pretrained=True)\n",
    "num_ftrs = resnet_model.fc.in_features\n",
    "resnet_model.fc = nn.Linear(num_ftrs, 11)\n",
    "resent_model = resnet_model.to(device)\n",
    "\n",
    "loss_module = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(resnet_model.parameters(), lr=0.001, momentum=0.9)\n",
    "num_epochs = 30\n",
    "batch_size = 32\n",
    "\n",
    "train_resnet18(resnet_model, optimizer, train_loader, loss_module, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Salvataggio del modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = resnet_model.state_dict()\n",
    "torch.save(state_dict, \"my_resnet_model.pt\")\n",
    "\n",
    "# Salva l'intero modello\n",
    "torch.save(resnet_model, 'entire_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valuatazione del modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data, loss_module):\n",
    "\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data:\n",
    "            \n",
    "            #inputs = inputs.to(device)\n",
    "            #labels = labels.to(device)\n",
    "\n",
    "            preds = model(inputs)\n",
    "            loss = loss_module(preds, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = preds.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuarcy:  100.0\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18()\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 11)\n",
    "model.load_state_dict(torch.load(\"my_resnet_model.pt\", map_location='cpu'))\n",
    "model.eval()\n",
    "loss_module = nn.CrossEntropyLoss()\n",
    "\n",
    "acc = evaluate_model(model, test_loader, loss_module)\n",
    "\n",
    "print(\"accuarcy: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparazione di resenet18 per l'estrazione di features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (7): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Caricamento di resnet18 per l'estrazione di features\n",
    "\n",
    "model = models.resnet18(weights=None)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 11)\n",
    "model.load_state_dict(torch.load(\"my_resnet_model.pt\", map_location='cpu'))\n",
    "\n",
    "model = nn.Sequential(*list(model.children()))[:-1]\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La seguente funzione utilizza resnet18 per ricavare il descrittore dell'immagine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appearance_descriptor(image):\n",
    "    with torch.no_grad():\n",
    "        features = model(image.unsqueeze(0))\n",
    "\n",
    "        features = torch.reshape(features, (-1,))\n",
    "\n",
    "        # Normalizziamo il vettore\n",
    "        sum = features.sum()\n",
    "        ft_norm = features/sum\n",
    "\n",
    "        ft_norm = ft_norm.numpy()\n",
    "\n",
    "        return ft_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepSORT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alcune funzioni ausiliari che saranno utili nell'elaborazione dei dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean = [x_center, y_center, aspect_ratio, height, Vx, Vy, Va, Vh] -> [x_min, y_min, width, height]\n",
    "def xy_min_wh(mean):\n",
    "    x_center, y_center = mean[0], mean[1]\n",
    "    aspect_ratio, height = mean[2], mean[3]\n",
    "\n",
    "    width = aspect_ratio * height\n",
    "    x_min, y_min = x_center - (width/2), y_center - (height/2)\n",
    "    new_bbox = np.array([x_min, y_min, width, height])\n",
    "    \n",
    "    return new_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bbox = [x_min, y_min, x_max, y-max] -> [x_center, y_center, a, height]\n",
    "def xy_center_ah(bbox):\n",
    "    x_min, y_min = bbox[0], bbox[1]\n",
    "    x_max, y_max = bbox[2], bbox[3]\n",
    "\n",
    "    width = x_max - x_min\n",
    "    height = y_max - y_min\n",
    "\n",
    "    x_center = x_min + width/2\n",
    "    y_center = y_min + height/2     \n",
    "\n",
    "    aspect_ratio = width/height    \n",
    "\n",
    "    new_bbox = bbox.copy()\n",
    "    new_bbox[0] = x_center\n",
    "    new_bbox[1] = y_center\n",
    "    new_bbox[2] = aspect_ratio\n",
    "    new_bbox[3] = height\n",
    "\n",
    "    return new_bbox\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(image, bbox):\n",
    "    # bbox = [x_min. y_min, x_max, y_max]\n",
    "    x_min, y_min = bbox[0], bbox[1]\n",
    "    x_max, y_max = bbox[2], bbox[3]\n",
    "\n",
    "    _, im_height, im_width = image.shape\n",
    "\n",
    "    if x_min < 0 or y_min < 0 or x_max >= im_width or y_max >= im_height or x_max - x_min <= 0 or y_max - y_min <= 0:\n",
    "        # in questo caso non c'è stata nessuna rilevazione e quindi lanciamo un errore\n",
    "        return image \n",
    "    \n",
    "    cropped_image = image[:, y_min:y_max, x_min:x_max]\n",
    "\n",
    "    return cropped_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kalman Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "INFTY_COST = 1e+5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Dizionario in cui sono salvati i valori dell'inverso del chi-quadro\n",
    "\"\"\"\n",
    "\n",
    "chi2inv95 = {\n",
    "    1: 3.8415,\n",
    "    2: 5.9915,\n",
    "    3: 7.8147,\n",
    "    4: 9.4877,\n",
    "    5: 11.070,\n",
    "    6: 12.592,\n",
    "    7: 14.067,\n",
    "    8: 15.507,\n",
    "    9: 16.919\n",
    "}\n",
    "\n",
    "class KalmanFilter:\n",
    "    def __init__(self):\n",
    "        ''' Per semplicita consideriamo un modello a velocità costante, quindi l'aspect ratio è costante.\n",
    "            La matrice dinamica da cui possiamo prevedere il prossimo stato è una matrice 8x8\n",
    "        '''\n",
    "\n",
    "        rows = 4\n",
    "        self.dynamic_matrix = np.eye(2*rows, 2*rows)\n",
    "        for i in range(rows):\n",
    "            self.dynamic_matrix[i, rows+i] = 1\n",
    "\n",
    "        ''' \n",
    "        La matrice 'update_pos' è necessaria per determinare la nuova posizione dell'oggetto \n",
    "        a partire dalle coordinate del bounding box\n",
    "        '''\n",
    "        self.update_pos = np.eye(rows, 2*rows)\n",
    "\n",
    "        self._std_weight_position = 1. / 20\n",
    "        self._std_weight_velocity = 1. / 160\n",
    "\n",
    "    def initiate(self, bounding_box):\n",
    "        ''' \n",
    "        Crea una nuova traccia per un rilevamento senza associazione\n",
    "        Bounding box (x_center, y_center, a, h) \n",
    "\n",
    "        '''\n",
    "\n",
    "        # Inizializziamo le velocita a zero, avremo quindi un vettore del tipo [x_center, y_center, a, h, 0, 0, 0, 0]\n",
    "        mean = bounding_box.copy()\n",
    "        for i in range(4):\n",
    "            mean = np.append(mean, 0)\n",
    "\n",
    "        # Covarianza sarà una matrice che misura l'incertezza della nostra stima\n",
    "\n",
    "        std = [\n",
    "            2 * self._std_weight_position * bounding_box[3],\n",
    "            2 * self._std_weight_position * bounding_box[3],\n",
    "            1e-2,\n",
    "            2 * self._std_weight_position * bounding_box[3],\n",
    "            10 * self._std_weight_velocity * bounding_box[3],\n",
    "            10 * self._std_weight_velocity * bounding_box[3],\n",
    "            1e-5,\n",
    "            10 * self._std_weight_velocity * bounding_box[3]\n",
    "        ]\n",
    "\n",
    "        covariance = np.diag(np.square(std))\n",
    "\n",
    "        return mean, covariance\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, mean, covariance):\n",
    "        mean = np.dot(self.dynamic_matrix, mean)\n",
    "        \n",
    "        # calcolo dell'incertezza\n",
    "        std_pos = [\n",
    "            self._std_weight_position * mean[3],\n",
    "            self._std_weight_position * mean[3],\n",
    "            1e-2,\n",
    "            self._std_weight_position * mean[3]]\n",
    "        std_vel = [\n",
    "            self._std_weight_velocity * mean[3],\n",
    "            self._std_weight_velocity * mean[3],\n",
    "            1e-5,\n",
    "            self._std_weight_velocity * mean[3]]\n",
    "        \n",
    "        motion_cov = np.diag(np.square(np.r_[std_pos, std_vel]))\n",
    "        \n",
    "        covariance = np.linalg.multi_dot((\n",
    "            self.dynamic_matrix, covariance, self.dynamic_matrix.T)) + motion_cov\n",
    "\n",
    "        return mean, covariance\n",
    "    \n",
    "    def project(self, mean, covariance):\n",
    "        std = [\n",
    "            self._std_weight_position * mean[3],\n",
    "            self._std_weight_position * mean[3],\n",
    "            1e-1,\n",
    "            self._std_weight_position * mean[3]\n",
    "        ]\n",
    "        innvoation_cov = np.diag(np.square(std))\n",
    "\n",
    "        mean = np.dot(self.update_pos, mean)\n",
    "\n",
    "        covariance = np.linalg.multi_dot((self.update_pos, covariance, self.update_pos.T))\n",
    "\n",
    "        return mean, covariance + innvoation_cov\n",
    "\n",
    "\n",
    "    def update(self, mean, covariance, measurement):\n",
    "        # Step di correzione della predizione\n",
    "\n",
    "        # bounding box = [x_center, y_center, a, h]\n",
    "        projected_mean, projected_cov = self.project(mean, covariance)\n",
    "\n",
    "        chol_factor, lower = scipy.linalg.cho_factor(\n",
    "            projected_cov, lower=True, check_finite=False)\n",
    "        kalman_gain = scipy.linalg.cho_solve(\n",
    "            (chol_factor, lower), np.dot(covariance, self.update_pos.T).T,\n",
    "            check_finite=False).T\n",
    "        innovation = measurement - projected_mean\n",
    "\n",
    "        new_mean = mean + np.dot(innovation, kalman_gain.T)\n",
    "        new_covariance = covariance - np.linalg.multi_dot((\n",
    "            kalman_gain, projected_cov, kalman_gain.T))\n",
    "        return new_mean, new_covariance\n",
    "\n",
    "        #projected_pred, projected_cov = self.project(prediction, covariance)\n",
    "#\n",
    "        #chol_factor, lower = scipy.linalg.cho_factor(\n",
    "        #    projected_cov, lower=True, check_finite=False)\n",
    "        #kalman_gain = scipy.linalg.cho_solve((chol_factor, lower), np.dot(covariance, self.update_pos.T).T, \n",
    "        #                                     check_finite=False).T\n",
    "        #innovation = bounding_box - projected_pred\n",
    "#\n",
    "        #new_pred = prediction + np.dot(innovation, kalman_gain.T)\n",
    "        #new_covariance = covariance - np.linalg.multi_dot((\n",
    "        #    kalman_gain, projected_cov, kalman_gain.T))\n",
    "        #\n",
    "        #return new_pred, new_covariance\n",
    "    \n",
    "    def gating_distance(self, mean, covariance, measurements, only_position=False):\n",
    "        \"\"\" \n",
    "        Funzione che calcola il quadrato della distanza di Mahalanobis\n",
    "        \"\"\"\n",
    "        mean, covariance = self.project(mean, covariance)\n",
    "        if only_position:\n",
    "            mean, covariance = mean[:2], covariance[:2, :2]\n",
    "            measurements = measurements[:, :2]\n",
    "\n",
    "        cholesky_factor = np.linalg.cholesky(covariance)\n",
    "        d = measurements - mean\n",
    "        z = scipy.linalg.solve_triangular(cholesky_factor, d.T, lower=True, check_finite=False, overwrite_b=True)\n",
    "        squared_maha = np.sum(z*z, axis= 0)\n",
    "\n",
    "        return squared_maha\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intersection-over-uninion (IOU_matching)\n",
    "L'obiettivo di questa funzione è trovare il miglior match fra il bounding box che rileva l'oggetto e i vari 'track' candidati. Più in concreto si vuole calcolare l'area del bbox occupata dal candidato e ovviamente maggiore è tale area e migliore è il candidato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(bbox, candidate):\n",
    "    ''' \n",
    "    bbox = [x_min, y_min, x_max, y_max]\n",
    "    '''\n",
    "    intersection = 0.0\n",
    "\n",
    "    # Coordinate del bbox\n",
    "    bx_min, by_min = bbox[0], bbox[1]\n",
    "    bx_max, by_max = bbox[2], bbox[3]\n",
    "\n",
    "    # Coordinate del candidato\n",
    "    cx_min, cy_min = candidate[0], candidate[1]\n",
    "    c_width, c_height = candidate[2], candidate[3]\n",
    "\n",
    "    bbox_area = (bx_max - bx_min) * (by_max - by_min)\n",
    "    candidate_area = c_width * c_height\n",
    "\n",
    "    x_min = max(bx_min, cx_min)\n",
    "    y_min = max(by_min, cy_min)\n",
    "\n",
    "    x_max = min(bx_max, cx_min+c_width)\n",
    "    y_max = min(by_max, cy_min+c_height)\n",
    "\n",
    "    intersection = (x_max - x_min) * (y_max - y_min)\n",
    "\n",
    "    return intersection / (bbox_area + candidate_area - intersection)\n",
    "\n",
    "\n",
    "def iou_cost(tracks, detections):\n",
    "\n",
    "    num_tracks = len(tracks)\n",
    "    num_dets = len(detections)\n",
    "\n",
    "    cost_matrix = np.zeros((num_tracks, num_dets))\n",
    "\n",
    "    for i in range(num_tracks):\n",
    "        if tracks[i].last_update > 1:\n",
    "            cost_matrix[i, :] = INFTY_COST\n",
    "            continue\n",
    "\n",
    "        mean = tracks[i].mean\n",
    "        bbox = xy_min_wh(mean)      # [x_min, y_min, e, h]\n",
    "        # max_score = 0.0\n",
    "        # index = -1\n",
    "\n",
    "        for j in range(num_dets):\n",
    "            candidate = detections[j]\n",
    "            score = iou(bbox, candidate)\n",
    "            cost_matrix[i, j] = 1. - score\n",
    "        #for j in range(num_dets):\n",
    "        #    candidate = detections[j]\n",
    "        #    score = iou(bbox, candidate) \n",
    "        #    if score > max_score:\n",
    "        #        max_score = score\n",
    "        #        index = j\n",
    "        #if index != -1:\n",
    "        #    cost_matrix[i, index] = 1. - max_score\n",
    "\n",
    "    return cost_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrackState:\n",
    "    Tentative = 1\n",
    "    Confirmed = 2\n",
    "    Deleted = 3\n",
    "    Suspended = 4   # MODIFICA\n",
    "\n",
    "\n",
    "class Track:\n",
    "\n",
    "    \"\"\" \n",
    "    Una traccia è composta dai seguenti attributi:\n",
    "        - mean = [x_center, y_center, a, h, Vx, Vy, Va, Vh]\n",
    "        - covariance: matrice 8x8 che serve a stimare l'incertezza della predizione\n",
    "        - track_id: identificativo della traccia\n",
    "        - n_init: numero di rilevamenti consecutivi prima che la traccia venga confermata\n",
    "        - max_age: massimo numero di volte consecutive senza associazione prima che la traccia\n",
    "                    venga eliminata\n",
    "        - feature: descrittore del rilevamento da cui la traccia è stata originata\n",
    "        - hits: numero totale di updates\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, covaraince, track_id, n_init, max_age, feature=None):\n",
    "        self.mean = mean\n",
    "        self.covariance = covaraince\n",
    "        self.track_id = track_id\n",
    "        self.n_init = n_init\n",
    "        self.max_age = max_age\n",
    "        self.state = TrackState.Tentative\n",
    "        #self.age = 1    # Inutilizzata\n",
    "        #self.class_index = class_index\n",
    "        self.hits = 1\n",
    "        self.last_update = 0\n",
    "        self.features = []\n",
    "        if feature is not None:\n",
    "            self.features.append(feature)\n",
    "\n",
    "    def predict(self, kf):\n",
    "        self.mean, self.covariance = kf.predict(self.mean, self.covariance)\n",
    "        # self.age += 1\n",
    "        self.last_update += 1\n",
    "        \n",
    "\n",
    "    def update(self, kf, detection, img):\n",
    "        # Aggiorna la traccia con una nuova rilevazione\n",
    "        self.mean, self.covariance = kf.update(self.mean, self.covariance, detection)\n",
    "        self.last_update = 0\n",
    "        self.hits += 1\n",
    "        if self.state == TrackState.Tentative and self.hits >= self.n_init:\n",
    "            self.state = TrackState.Confirmed\n",
    "        # MODIFICA\n",
    "        elif self.state == TrackState.Suspended:\n",
    "            self.state = TrackState.Confirmed\n",
    "\n",
    "        # estraiamo l'immagine delineata dal bounding box e ne calcoliamo il descrittore\n",
    "        cropped_image = crop_image(img, detection)\n",
    "        feature = appearance_descriptor(cropped_image)\n",
    "        self.features.append(feature)\n",
    "        \n",
    "\n",
    "    def mark_missed(self):\n",
    "        # Gestisce il caso in cui la traccia non viene associata ad una rilevazione\n",
    "        if self.state == TrackState.Tentative:\n",
    "            self.state = TrackState.Deleted\n",
    "\n",
    "        elif self.last_update > self.max_age:\n",
    "            self.state = TrackState.Deleted\n",
    "        # MODIFICA\n",
    "        else:\n",
    "            self.state = TrackState.Suspended\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NearestNeigbhorDistanceMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(a, b):\n",
    "    return np.dot(a, b.T)\n",
    "    #return 1. - np.dot(a, b.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_cosine_distance(x, y):\n",
    "    distances = cosine_distance(x, y)\n",
    "    return distances.min(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NearestNeighborDistanceMetric:\n",
    "\n",
    "    def __init__(self, metric, matching_threshold):\n",
    "        \n",
    "        self.metric = metric\n",
    "        self.matching_threshold = matching_threshold\n",
    "        self.samples = {}\n",
    "\n",
    "    def partial_fit(self, features, targets, active_tagets):\n",
    "        # Funzione che ad ogni target associa le sue features\n",
    "        for feature, target in zip(features, targets):\n",
    "            self.samples.setdefault(target, []).append(feature)\n",
    "\n",
    "        self.samples = {k: self.samples[k] for k in active_tagets}\n",
    "\n",
    "\n",
    "    def distance(self, features, targets):\n",
    "        num_features = len(features)\n",
    "        num_targets = len(targets)\n",
    "\n",
    "        # print(self.samples)\n",
    "\n",
    "        cost_matrix = np.zeros((num_targets, num_features))\n",
    "\n",
    "        for i in range(num_targets):\n",
    "            target = targets[i]\n",
    "            cost_matrix[i, :] = self.metric(self.samples[target], features)\n",
    "\n",
    "        #print(cost_matrix)\n",
    "        return cost_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_cost_matching(distance_metric, max_distance, tracks, detections, img):\n",
    "\n",
    "    num_tracks = len(tracks)\n",
    "    num_detections = len(detections)\n",
    "\n",
    "    cost_matrix = np.zeros((num_tracks, num_detections))\n",
    "    if distance_metric == iou_cost:\n",
    "        cost_matrix = distance_metric(tracks, detections)\n",
    "    else:\n",
    "        cost_matrix = distance_metric(tracks, detections, img)\n",
    "\n",
    "    cost_matrix[cost_matrix > max_distance] = max_distance + 1e-5\n",
    "        \n",
    "    matches = []\n",
    "\n",
    "    tracks_idx, dets_idx = linear_sum_assignment(cost_matrix)\n",
    "\n",
    "    for row, col in zip(tracks_idx, dets_idx):\n",
    "        track = tracks[row]\n",
    "        det = detections[col]\n",
    "\n",
    "        if cost_matrix[row, col] < max_distance:\n",
    "            matches.append((track, det))\n",
    "\n",
    "    return matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching_cascade(distance_metric, max_distance, tracks, detections, Amax, img):\n",
    "    # distance_metric: funzione che prende in ingresso una lista di track \n",
    "    # e una lista di rilevamento e ritorna la matrice di costo, ovvero in \n",
    "    # posizione (i, j) troviamo il costo dell'associazione fra la traccia \n",
    "    # i ed il rilevamento j\n",
    "\n",
    "    # max_distance: associazioni con un costo maggiore di max_distance vengono scartate\n",
    "    \n",
    "    num_tracks = len(tracks)\n",
    "    num_detections = len(detections)\n",
    "\n",
    "    if num_tracks == 0 or num_detections == 0:\n",
    "        return []\n",
    "    \n",
    "    M = []\n",
    "    U = detections.copy()\n",
    "\n",
    "    for n in range(Amax):\n",
    "\n",
    "        Tn = list()\n",
    "        for i in range(num_tracks):\n",
    "            if tracks[i].last_update == n + 1:\n",
    "                Tn.append(tracks[i])\n",
    "\n",
    "        if len(Tn) == 0:\n",
    "            continue\n",
    "\n",
    "        matches = min_cost_matching(distance_metric, max_distance, Tn, U, img)\n",
    "\n",
    "        matched_dets = [d for _, d in matches]\n",
    "\n",
    "        M += matches\n",
    "\n",
    "        matched_dets = [d for _, d in matches]\n",
    "\n",
    "        set1 = set(tuple(arr.flatten()) for arr in U)\n",
    "        set2 = set(tuple(arr.flatten()) for arr in matched_dets)\n",
    "        difference = set(set1 - set2)\n",
    "\n",
    "        if len(difference) == 0:\n",
    "            break\n",
    "\n",
    "        U = [np.array(arr) for arr in difference]\n",
    "\n",
    "\n",
    "    return M\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gate_cost_matrix(cost_matrix, tracks, detections, kf, only_position=False):\n",
    "    gate_dim = 0\n",
    "    if only_position:\n",
    "        gate_dim = 2\n",
    "    else:\n",
    "        gate_dim = 4\n",
    "\n",
    "    measurements = np.array(detections)\n",
    "\n",
    "    threshold = chi2inv95[gate_dim]\n",
    "    num_tracks = len(tracks)\n",
    "    for i in range(num_tracks):\n",
    "        track = tracks[i]\n",
    "        cost = kf.gating_distance(track.mean, track.covariance, measurements, only_position)\n",
    "        cost_matrix[i, cost > threshold] = INFTY_COST\n",
    "\n",
    "    return cost_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tracker:\n",
    "    def __init__(self, metric, max_iou_distance=0.7, n_init=3, max_age=30):\n",
    "        self.metric = metric\n",
    "        self.max_iou_distance = max_iou_distance\n",
    "        self.n_init = n_init \n",
    "        self.max_age = max_age\n",
    "        self.tracks = []\n",
    "        self.track_id = 1\n",
    "        self.kf = KalmanFilter()\n",
    "\n",
    "    def predict(self):\n",
    "        # Per ogni traccia attiva nel frame attuale\n",
    "        # prediciamo la sua pposizione nel frame successivo\n",
    "\n",
    "        num_tracks = len(self.tracks)\n",
    "        for i in range(num_tracks):\n",
    "            self.tracks[i].predict(self.kf)\n",
    "            \n",
    "    def update(self, detections, img):\n",
    "        # In questo step andiamo a correggere le \n",
    "        # predizioni fatte nello step precedente\n",
    "\n",
    "        matches = self.match(detections, img)\n",
    "\n",
    "        for track, det in matches:\n",
    "            # trasformiamo le coordinate del bounding box:\n",
    "            # [x_min, y_min, x_max, y_max] -> [x_center, y_center, a, h]\n",
    "            measurement = xy_center_ah(det)\n",
    "            track.update(self.kf, measurement, img)\n",
    "\n",
    "        matched_tracks = [t for t, _ in matches]\n",
    "        matched_dets = [d for _, d in matches]\n",
    "\n",
    "\n",
    "        list1 = [tuple(arr.flatten()) for arr in matched_dets]\n",
    "        list2 = [tuple(arr.flatten()) for arr in detections]\n",
    "    \n",
    "        unmatched_dets = [np.array(d) for d in list2 if d not in list1]\n",
    "\n",
    "        for t in self.tracks:\n",
    "            if t not in matched_tracks:\n",
    "                t.mark_missed()\n",
    "\n",
    "        for det in unmatched_dets:\n",
    "            self.add_track(det, img)\n",
    "\n",
    "\n",
    "        self.tracks = [t for t in self.tracks if t.state != TrackState.Deleted]\n",
    "\n",
    "        #confirmed_tracks = [t for t in self.tracks if t.state == TrackState.Confirmed] \n",
    "\n",
    "\n",
    "        # Aggiorna distance metric\n",
    "\n",
    "        active_targets = [t.track_id for t in self.tracks if t.state != TrackState.Deleted]\n",
    "        features, targets = [], []\n",
    "        for track in self.tracks:\n",
    "            if track.state == TrackState.Deleted:\n",
    "                continue\n",
    "\n",
    "            features += track.features\n",
    "            targets += [track.track_id for _ in track.features]\n",
    "            track.features = []\n",
    "\n",
    "        self.metric.partial_fit(np.asarray(features), np.asarray(targets), active_targets)\n",
    "\n",
    "\n",
    "        \n",
    "    def match(self, detections, img):\n",
    "\n",
    "        def gated_metric(tracks, detections, img):\n",
    "            # Calcoliamo la matrice dei costi relativamente alla cosine_distance\n",
    "            num_tracks = len(tracks)\n",
    "            num_detections = len(detections)\n",
    "            cropped_images = []\n",
    "            for i in range(num_detections):\n",
    "                cropped_img = crop_image(img, detections[i])\n",
    "                cropped_images.append(cropped_img)\n",
    "\n",
    "            descriptors = np.array([appearance_descriptor(cropped_images[i]) for i in range(len(cropped_images))])\n",
    "\n",
    "            targets = np.array([tracks[i].track_id for i in range(num_tracks)])\n",
    "            cost_matrix = self.metric.distance(descriptors, targets)\n",
    "\n",
    "            #print(cost_matrix)          \n",
    "\n",
    "            # trasformiamo le coordinate del bounding box in [x_center, y_center, a, h]\n",
    "            dets = [xy_center_ah(d) for d in detections]\n",
    "            cost_matrix = gate_cost_matrix(cost_matrix, tracks, dets, self.kf)\n",
    "\n",
    "            return cost_matrix\n",
    "        \n",
    "\n",
    "        \n",
    "        #confirmed_tracks = [t for t in self.tracks if t.state == TrackState.Confirmed] \n",
    "\n",
    "        unconfirmed_tracks = [t for t in self.tracks if t.state != TrackState.Confirmed]\n",
    "        \n",
    "        matches = matching_cascade(gated_metric, self.metric.matching_threshold, self.tracks, detections, self.max_age, img)\n",
    "    \n",
    "        # self.matching_threshold è la max_cosine_distance\n",
    "\n",
    "        # if len(matches) > 0:\n",
    "        #     print(\"FOUND\")\n",
    "\n",
    "        matched_tracks = [t for t,_ in matches]\n",
    "        matched_dets = [d for _, d in matches]\n",
    "\n",
    "        list1 = [tuple(arr.flatten()) for arr in matched_dets]\n",
    "        list2 = [tuple(arr.flatten()) for arr in detections]\n",
    "    \n",
    "        unmatched_dets = [np.array(d) for d in list2 if d not in list1]\n",
    "        unmatched_tracks = [t for t in self.tracks if t not in matched_tracks]\n",
    "\n",
    "\n",
    "        # Facciamo un ultimo tentativo di associazione per le tracce di età 1, utilizzando IOU\n",
    "        new_tentative_tracks = list(set([t for t in unconfirmed_tracks if t.last_update == 1] + \\\n",
    "            [t for t in unmatched_tracks if t.last_update == 1]))\n",
    "\n",
    "        new_matches = matching_cascade(iou_cost, self.max_iou_distance, new_tentative_tracks, unmatched_dets, self.max_age, img)\n",
    "\n",
    "        total_matches = matches + new_matches\n",
    "\n",
    "        return total_matches\n",
    "\n",
    "\n",
    "    def add_track(self, detection, img):\n",
    "        # inizializziamo una nuova traccia per ogni rilevamento senza associazione\n",
    "        # inoltre calcoliamo un descrittore per il rileavmento da cui stiamo generando la traccia\n",
    "        #class_index = detection[4]\n",
    "        #detection = detection[:4]\n",
    "        \n",
    "        cropped_image = crop_image(img, detection)\n",
    "        descriptor = appearance_descriptor(cropped_image)\n",
    "\n",
    "        det = xy_center_ah(detection) \n",
    "        \n",
    "        mean, cov = self.kf.initiate(det)\n",
    "        new_track = Track(mean, cov, self.track_id, self.n_init, self.max_age, feature=descriptor)\n",
    "    \n",
    "        # self.class_index = class_index\n",
    "        self.track_id += 1\n",
    "        self.tracks.append(new_track)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Esecuzione di DeepSORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class run_deepSORT():\n",
    "    def __init__(self, video):\n",
    "        self.video = video\n",
    "        self.model = self.load_model()\n",
    "        self.classes = self.model.model.names\n",
    "\n",
    "    def load_model(self):\n",
    "        model = YOLO('best.pt')\n",
    "        #model.eval()\n",
    "\n",
    "        return model \n",
    "    \n",
    "    def predict(self, img):\n",
    "        results = self.model.predict(img, verbose=False)\n",
    "        return results \n",
    "    \n",
    "    def get_boxes(self, results, img):\n",
    "        detections = []\n",
    "\n",
    "        for r in results:\n",
    "            boxes = r.boxes\n",
    "            for box in boxes:\n",
    "                x_min, y_min, x_max, y_max = box.xyxy[0]\n",
    "\n",
    "                class_index = int(box.cls[0])\n",
    "                #class_name = self.classes[class_index]\n",
    "\n",
    "                conf = box.conf[0]\n",
    "\n",
    "                if conf > 0.5:\n",
    "                    detections.append(np.array([int(x_min), int(y_min), int(x_max), int(y_max)]))    \n",
    "\n",
    "        return detections, img\n",
    "    \n",
    "    def track_detect(self, detections, img, tracker):\n",
    "        # prediciamo la posizione delle tracce attive\n",
    "        tracker.predict()\n",
    "\n",
    "        # trasformiamo l'immagine in un tensore\n",
    "        rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        pil_image = Image.fromarray(rgb_img)\n",
    "        img_tensor = test_transform(pil_image)\n",
    "\n",
    "        # associamo le predizioni ai rilevamenti\n",
    "        tracker.update(detections, img_tensor)\n",
    "\n",
    "        # Estraiamo le tracce\n",
    "        tracks = tracker.tracks\n",
    "\n",
    "        for track in tracks:\n",
    "            if track.state != TrackState.Confirmed:\n",
    "                continue \n",
    "         \n",
    "\n",
    "            track_id = track.track_id \n",
    "            # class_index = track.class_index\n",
    "            # name = self.classes[class_index]\n",
    "            mean = track.mean\n",
    "            \n",
    "            #state = mean[:4]\n",
    "            #print(\"state: \", state)\n",
    "\n",
    "            bbox = xy_min_wh(mean)\n",
    "\n",
    "            #print(bbox)\n",
    "\n",
    "            x_min, y_min = int(bbox[0]), int(bbox[1])\n",
    "            width, height = int(bbox[2]), int(bbox[3])\n",
    "\n",
    "            cvzone.putTextRect(img, f'ID: {track_id}', (x_min, y_min), scale=1, thickness=1, colorR=(0,0,255))\n",
    "            cvzone.cornerRect(img, (x_min, y_min, width, height), l=9, rt=1, colorR=(255,0,255))\n",
    "\n",
    "        return img\n",
    "        \n",
    "    def run_tracker(self):\n",
    "        cap = cv2.VideoCapture(self.video)\n",
    "\n",
    "        max_cosine_distance = 0.2 # Default(0.2)\n",
    "\n",
    "        metric = NearestNeighborDistanceMetric(nn_cosine_distance, max_cosine_distance)\n",
    "\n",
    "        tracker = Tracker(metric)\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, img = cap.read()\n",
    "\n",
    "            if ret == True:\n",
    "\n",
    "                results = self.predict(img)\n",
    "                detections, frame = self.get_boxes(results, img)\n",
    "                # print(detections)\n",
    "                detect_frame = self.track_detect(detections, frame, tracker)\n",
    "\n",
    "                cv2.imshow('Image', detect_frame)\n",
    "                if cv2.waitKey(1) == ord('q'):\n",
    "                    break\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = run_deepSORT(\"Tracking_test/Ballena/Ballena.mp4\")\n",
    "tracker.run_tracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = run_deepSORT(\"Tracking_test/Octopus2/Octopus2.mp4\")\n",
    "tracker.run_tracker()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
