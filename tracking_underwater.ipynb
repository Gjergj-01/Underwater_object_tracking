{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.utils.data as data \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import transforms, datasets, models\n",
    "from torchvision.io import read_image, read_video\n",
    "\n",
    "## Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Yolo\n",
    "from ultralytics import YOLO\n",
    "\n",
    "import scipy.linalg\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparazione dei dati\n",
    "Il dataset originario era composto da 26067 immagini, mentre per motivi logistici in questo notebook è stato ridotto a 9014 immagini e 11 classi (invece delle 32 originarie). Per carciare le immagini usiamo la funzionalità `ImageLoader` presente in Pytorch che permette di estrarre tutte le immagini presenti in una cartella (e nelle sue sottocartelle) e assegna come lable a tali immagini il nome della cartella stessa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../archive\"\n",
    "\n",
    "class ImageFolderWithIndices(datasets.ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        image, label = super().__getitem__(index)\n",
    "\n",
    "        path = self.imgs[index][0]\n",
    "        filename = os.path.basename(path)\n",
    "\n",
    "        str_num = ''\n",
    "        for elem in filename:\n",
    "            if elem.isdigit():\n",
    "                str_num += elem\n",
    "\n",
    "        num = (int) (str_num)\n",
    "    \n",
    "        return image, label, num\n",
    "    \n",
    "dataset = ImageFolderWithIndices(root=dataset_path)\n",
    "#print(dataset.classes)\n",
    "#print(len(dataset))\n",
    "\n",
    "'''\n",
    "    Il dataset viene suddiviso nel modo seguente:\n",
    "        - 80% training_set\n",
    "        - 10% validation_set\n",
    "        - 10% test_set\n",
    "'''\n",
    "\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(dataset, [7212, 901, 901])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estrazione dei bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "bbox = [x_top_left, y_top_left, width, height, index], dove index è l'indice dell'immagine nella cartella in cui è salvata,\n",
    "questo serve per ricreare la corrispondenza fra immagine e lable\n",
    "'''\n",
    "\n",
    "bounding_boxes = []\n",
    "\n",
    "entries = sorted(os.listdir(dataset_path))\n",
    "for folder in entries:\n",
    "    sub_directory = os.path.join(dataset_path, folder)\n",
    "    tensor_list = []\n",
    "\n",
    "    for filename in os.listdir(sub_directory):\n",
    "        if filename.endswith(\"groundtruth_rect.txt\"):\n",
    "            path = os.path.join(sub_directory, filename)\n",
    "            f = open(path, \"r\")\n",
    "            lines = f.readlines()\n",
    "            len_file = len(lines)\n",
    "\n",
    "            for i in range(len_file):\n",
    "                line = lines[i].split()\n",
    "                floates = [float(x) for x in line]\n",
    "                index = (float)(i)\n",
    "                floates.append(index)\n",
    "                coordinates = torch.tensor(floates)\n",
    "                tensor_list.append(coordinates)\n",
    "        \n",
    "    bounding_boxes.append(tensor_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparazione del dataset per l'addestramento di YOLOv8 e ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"datasets/dataset\"\n",
    "resnet_dir = \"resnet_dir\"\n",
    "\n",
    "# creazione delle directory per Yolov8\n",
    "\n",
    "image_dirs = {\n",
    "    \"train\" : os.path.join(dataset_dir, \"images/train\"),\n",
    "    \"val\": os.path.join(dataset_dir, \"images/val\"),\n",
    "    \"test\": os.path.join(dataset_dir, \"images/test\")\n",
    "}\n",
    "\n",
    "label_dirs = {\n",
    "    \"train\": os.path.join(dataset_dir, \"labels/train\"),\n",
    "    \"val\": os.path.join(dataset_dir, \"labels/val\"),\n",
    "    \"test\": os.path.join(dataset_dir, \"labels/test\")\n",
    "}\n",
    "\n",
    "# Creazione delle directory per ResNet50\n",
    "\n",
    "dirs = {\n",
    "    \"train\" : os.path.join(resnet_dir, \"train\"),\n",
    "    \"val\" : os.path.join(resnet_dir, \"val\"),\n",
    "    \"test\" : os.path.join(resnet_dir, \"test\")\n",
    "}\n",
    "\n",
    "# Creazione delle cartelle\n",
    "\n",
    "for dir_path in image_dirs.values():\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "for dir_path in label_dirs.values():\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "for dir_path in dirs.values():\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# Salvataggio delle delle immagini e dei bounding boxes\n",
    "\n",
    "def save_images_and_labels(data, dataset_type):\n",
    "    image_dir = image_dirs[dataset_type]\n",
    "    label_dir = label_dirs[dataset_type]\n",
    "    cropped_image_dir = dirs[dataset_type]\n",
    "\n",
    "    for image, label, index in data:\n",
    "        img_save_path = os.path.join(image_dir, f\"{index}_{label}.jpg\")\n",
    "        image.save(img_save_path)\n",
    "\n",
    "        width = image.width\n",
    "        height = image.height\n",
    "\n",
    "        bounding_box = bounding_boxes[label][index-1]\n",
    "        bbox_width = bounding_box[2]\n",
    "        bbox_height = bounding_box[3]\n",
    "        \n",
    "        class_id = label\n",
    "\n",
    "        label_save_path = os.path.join(label_dir, f\"{index}_{label}.txt\")\n",
    "        with open(label_save_path, 'w') as f:\n",
    "\n",
    "            # normalizziamo il bbox\n",
    "\n",
    "            x_center = (bounding_box[0] + bbox_width/2) / width\n",
    "            y_center = (bounding_box[1] + bbox_height/2) / height\n",
    "            b_width = bbox_width / width\n",
    "            b_height = bbox_height / height\n",
    "\n",
    "            f.write(f\"{class_id} {x_center} {y_center} {b_width} {b_height}\\n\")\n",
    "\n",
    "        # Ritagliamo le immagini tenendoci solo la parte individuata dai bounding boxes\n",
    "\n",
    "        bbox_width = float(bbox_width)\n",
    "        bbox_height = float(bbox_height)\n",
    "        \n",
    "        x_min, y_min = round(float(bounding_box[0]), 1), round(float(bounding_box[1]), 1)\n",
    "        x_max = round(float(x_min + bbox_width), 1)\n",
    "        y_max = round(float(y_min + bbox_height), 1)\n",
    "\n",
    "        # Check che verifica che non si esca dai bordi dell'immagine o che non ci siano bounding boxes con \n",
    "        # altezza o spessore nullo\n",
    "        if x_min < 0 or y_min < 0 or x_max >= width or y_max >= height or x_max - x_min <= 0 or y_max - y_min <= 0:\n",
    "            continue\n",
    "        \n",
    "        cropped_image = image.crop((x_min, y_min, x_max, y_max))\n",
    "        cropped_save_path = os.path.join(cropped_image_dir, f\"{index}_{label}.jpg\")\n",
    "        cropped_image.save(cropped_save_path)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_images_and_labels(train_set, \"train\")\n",
    "save_images_and_labels(test_set, \"test\")\n",
    "save_images_and_labels(val_set, \"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yolov8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.train(data=\"file.yaml\", epochs=15, batch=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(\"datasets/dataset/images/test/10_7.jpg\", save=False, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracking con Yolov8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = model.track(source=\"Tracking_test/Ballena/Ballena.mp4\", conf=0.3, iou=0.6, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = model.track(source=\"Tracking_test/SeaDiver/SeaDiver.mp4\", conf=0.3, iou=0.5, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res3 = model.track(source=\"Tracking_test/Octopus2/Octopus2.mp4\", conf=0.3, iou=0.5, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Addestramento di ResNet50 per l'estrazione delle feature dai bounding boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nelle prime due celle di codice estraiamo le immagini per al fine di calcolare la loro media e deviazione standard per poterle normalizzare in modo che complessivamente la media e la deviazione standard su tutto il dataset siano approssimativamente 0 e 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "tmp_dataset = datasets.ImageFolder(root=resnet_dir, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([0.2522, 0.4443, 0.4805])\n",
      "std: tensor([0.1188, 0.1406, 0.1375])\n"
     ]
    }
   ],
   "source": [
    "# Andiamo a calcolare la media e la deviazione standard per \n",
    "# poter poi normalizzare le immagini\n",
    "\n",
    "dataset_size = len(tmp_dataset)\n",
    "\n",
    "DATA_MEAN = 0.0\n",
    "var = 0.0\n",
    "\n",
    "for i in range(dataset_size):\n",
    "    image, _ = tmp_dataset[i]\n",
    "    DATA_MEAN += image.mean(dim=(1, 2))\n",
    "    var += image.var(dim=(1, 2))\n",
    "    \n",
    "\n",
    "DATA_MEAN /= dataset_size\n",
    "DATA_STD = torch.sqrt(var/dataset_size)\n",
    "\n",
    "print(f\"Mean: {DATA_MEAN}\")\n",
    "print(f\"std: {DATA_STD}\")\n",
    "\n",
    "\"\"\" \n",
    "Mean: tensor([0.2522, 0.4443, 0.4805])\n",
    "std: tensor([0.1188, 0.1406, 0.1375])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nmodel.fc = nn.Identity()\\n\\nfeatures = model(image)\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.2522, 0.4443, 0.4805], [0.1188, 0.1406, 0.1375])\n",
    "])\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.2522, 0.4443, 0.4805], [0.1188, 0.1406, 0.1375])\n",
    "])\n",
    "\n",
    "train_dataset_path = \"resnet_dir/train1/train\"\n",
    "val_dataset_path = \"resnet_dir/val1/val\"\n",
    "test_dataset_path = \"resnet_dir/test1/test\"\n",
    "\n",
    "class CostumImageDataset(Dataset):\n",
    "    def __init__(self, image_folder, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.image_filenames = os.listdir(image_folder)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_folder, self.image_filenames[idx])\n",
    "        image = Image.open(img_path)\n",
    "        filename = self.image_filenames[idx].split('_')\n",
    "        category = filename[1].split('.')\n",
    "        label = int(category[0])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return (image, label)\n",
    "    \n",
    "\n",
    "train_dataset = CostumImageDataset(train_dataset_path, train_transform)\n",
    "val_dataset = CostumImageDataset(val_dataset_path, test_transform)\n",
    "test_dataset = CostumImageDataset(val_dataset_path, test_transform)\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "model.fc = nn.Identity()\n",
    "\n",
    "features = model(image)\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch mean tensor([-0.1059, -0.0629,  0.0056])\n",
      "Batch std tensor([1.3584, 1.3395, 1.6535])\n"
     ]
    }
   ],
   "source": [
    "imgs, _ = next(iter(train_loader))\n",
    "print(\"Batch mean\", imgs.mean(dim=[0,2,3]))\n",
    "print(\"Batch std\", imgs.std(dim=[0,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_resnet18(model, optimizer, data_loader, loss_module, num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    # Parallelize training accross multiple GPUs\n",
    "    # model = torch.nn.DataParallel(model)\n",
    "\n",
    "    loss = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, labels in data_loader:\n",
    "            # Questo passaggio è strettamente necessrio solo se si usa una gpu\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            preds = model(inputs)\n",
    "            preds = preds.squeeze(dim=1)\n",
    "\n",
    "            loss = loss_module(preds, labels)\n",
    "\n",
    "            # Prima di calcolare i gradienti ci assicuriamo che siano tutti zero\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Aggiornamento dei parametri\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model = models.resnet18(pretrained=True)\n",
    "num_ftrs = resnet_model.fc.in_features\n",
    "resnet_model.fc = nn.Linear(num_ftrs, 11)\n",
    "resent_model = resnet_model.to(device)\n",
    "\n",
    "loss_module = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(resnet_model.parameters(), lr=0.001, momentum=0.9)\n",
    "num_epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "train_resnet18(resnet_model, optimizer, train_loader, loss_module, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Salvataggio del modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = resnet_model.state_dict()\n",
    "torch.save(state_dict, \"my_resnet_model.pt\")\n",
    "\n",
    "\"\"\"\n",
    "# Specify a path\n",
    "PATH = \"state_dict_model.pt\"\n",
    "\n",
    "# Save\n",
    "torch.save(net.state_dict(), PATH)\n",
    "\n",
    "# Load\n",
    "model = Net()\n",
    "model.load_state_dict(torch.load(PATH, weights_only=True))\n",
    "model.eval()\n",
    "\"\"\"\n",
    "\n",
    "# Salva l'intero modello\n",
    "torch.save(resnet_model, 'entire_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valuatazione del modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data, loss_module):\n",
    "\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data:\n",
    "            \n",
    "            #inputs = inputs.to(device)\n",
    "            #labels = labels.to(device)\n",
    "\n",
    "            preds = model(inputs)\n",
    "            loss = loss_module(preds, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = preds.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuarcy:  100.0\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18()\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 11)\n",
    "model.load_state_dict(torch.load(\"my_resnet_model.pt\", map_location='cpu'))\n",
    "model.eval()\n",
    "loss_module = nn.CrossEntropyLoss()\n",
    "\n",
    "acc = evaluate_model(model, test_loader, loss_module)\n",
    "\n",
    "print(\"accuarcy: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparazione di resenet18 per l'estrazione di features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9983315977733582\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(weights=None)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 11)\n",
    "model.load_state_dict(torch.load(\"my_resnet_model.pt\", map_location='cpu'))\n",
    "\n",
    "model = nn.Sequential(*list(model.children()))[:-1]\n",
    "model.eval()\n",
    "images, _ = next(iter(test_loader))\n",
    "image1 = images[0]\n",
    "image2 = images[1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    features1 = model(image1.unsqueeze(0))\n",
    "    features2 = model(image2.unsqueeze(0))\n",
    "\n",
    "    features1 = torch.reshape(features1, (-1,))\n",
    "    features2 = torch.reshape(features2, (-1,))\n",
    "\n",
    "    # Normalizziamo i due vettori\n",
    "    sum1 = features1.sum()\n",
    "    sum2 = features2.sum()\n",
    "\n",
    "    ft1_norm = features1/sum1\n",
    "    ft2_norm = features2/sum2\n",
    "\n",
    "    ft1_norm = ft1_norm.numpy()\n",
    "    ft2_norm = ft2_norm.numpy()\n",
    "\n",
    "print(1 - np.dot(ft1_norm, ft2_norm.T))\n",
    "\n",
    "#print(1 - torch.dot(ft1_norm, torch.t(ft2_norm)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La seguente funzione ritorna i due descrittori delle immagini di cui poi vogliamo calcolare la *cosine similarity*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_descriptors(image1, image2):\n",
    "    with torch.no_grad():\n",
    "        features1 = model(image1.unsqueeze(0))\n",
    "        features2 = model(image2.unsqueeze(0))\n",
    "\n",
    "        features1 = torch.reshape(features1, (-1,))\n",
    "        features2 = torch.reshape(features2, (-1,))\n",
    "\n",
    "        # Normalizziamo i due vettori\n",
    "        sum1 = features1.sum()\n",
    "        sum2 = features2.sum()\n",
    "\n",
    "        ft1_norm = features1/sum1\n",
    "        ft2_norm = features2/sum2\n",
    "\n",
    "        ft1_norm = ft1_norm.numpy()\n",
    "        ft2_norm = ft2_norm.numpy()\n",
    "\n",
    "        return ft1_norm, ft2_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepSORT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kalman Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Dizionario in cui sono salvati i valori dell'inverso del chi-quadro\n",
    "\"\"\"\n",
    "\n",
    "chi2inv95 = {\n",
    "    1: 3.8415,\n",
    "    2: 5.9915,\n",
    "    3: 7.8147,\n",
    "    4: 9.4877,\n",
    "    5: 11.070,\n",
    "    6: 12.592,\n",
    "    7: 14.067,\n",
    "    8: 15.507,\n",
    "    9: 16.919\n",
    "}\n",
    "\n",
    "class KalmanFiletr:\n",
    "    def __init__(self):\n",
    "        ''' Per semplicita consideriamo un modello a velocità costante, quindi l'aspect ratio è costante.\n",
    "            La matrice dinamica da cui possiamo prevedere il prossimo stato è una matrice 8x8\n",
    "        '''\n",
    "\n",
    "        rows = 4\n",
    "        self.dynamic_matrix = np.eye(2*rows, 2*rows)\n",
    "        for i in range(rows):\n",
    "            self.dynamic_matrix[i, rows+i] = 1\n",
    "\n",
    "        ''' \n",
    "        La matrice 'update_pos' è necessaria per determinare la nuova posizione dell'oggetto \n",
    "        a partire dalle coordinate del bounding box\n",
    "        '''\n",
    "        self.update_pos = np.eye(rows, 2*rows)\n",
    "\n",
    "        self._std_weight_position = 1. / 20\n",
    "        self._std_weight_velocity = 1. / 160\n",
    "\n",
    "    def initiate(self, bounding_box):\n",
    "        ''' \n",
    "        Crea una nuova traccia per un rilevamento senza associazione\n",
    "        Bounding box (x_center, y_center, a, h) \n",
    "\n",
    "        returns:\n",
    "            the mean vector of the new track (8 dimensional) and the covariance \n",
    "            matrix (8 x 8 dimensional) of the new track \n",
    "        '''\n",
    "\n",
    "        # ATTENZIONE alle coordinate e all'aspect ration, il bounding box restituito da Yolov8 è: [x_min, y_min, width, height]\n",
    "\n",
    "        # Inizializziamo le velocita a zero, avremo quindi un vettore del tipo [x_center, y_center, a, h, 0, 0, 0, 0]\n",
    "        mean = bounding_box\n",
    "        for i in range(4):\n",
    "            mean = mean.append(0)\n",
    "\n",
    "        # Covarianza sarà una matrice che misura l'incertezza della nostra stima\n",
    "\n",
    "        std = [\n",
    "            2 * self._std_weight_position * bounding_box[3],\n",
    "            2 * self._std_weight_position * bounding_box[3],\n",
    "            1e-2,\n",
    "            2 * self._std_weight_position * bounding_box[3],\n",
    "            10 * self._std_weight_velocity * bounding_box[3],\n",
    "            10 * self._std_weight_velocity * bounding_box[3],\n",
    "            1e-5,\n",
    "            10 * self._std_weight_velocity * bounding_box[3]\n",
    "        ]\n",
    "\n",
    "        covariance = np.diag(np.square(std))\n",
    "\n",
    "        return mean, covariance\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, mean, covariance):\n",
    "        prediction = np.dot(self.dynamic_matrix, mean)\n",
    "        \n",
    "        # calcolo dell'incertezza\n",
    "        std_pos = [\n",
    "            self._std_weight_position * mean[3],\n",
    "            self._std_weight_position * mean[3],\n",
    "            1e-2,\n",
    "            self._std_weight_position * mean[3]]\n",
    "        std_vel = [\n",
    "            self._std_weight_velocity * mean[3],\n",
    "            self._std_weight_velocity * mean[3],\n",
    "            1e-5,\n",
    "            self._std_weight_velocity * mean[3]]\n",
    "        \n",
    "        motion_covariance = np.diag(np.square(np.r_[std_pos, std_vel]))\n",
    "        \n",
    "        uncertainity = np.linalg.multi_dot((self.dynamic_matrix, covariance, self.dynamic_matric.T)) + motion_covariance\n",
    "\n",
    "        return prediction, uncertainity\n",
    "    \n",
    "    def project(self, mean, covariance):\n",
    "        std = [\n",
    "            self._std_weight_position * mean[3],\n",
    "            self._std_weight_position * mean[3],\n",
    "            1e-1,\n",
    "            self._std_weight_position * mean[3]\n",
    "        ]\n",
    "        innvoation_cov = np.diag(np.square(std))\n",
    "\n",
    "        mean = np.dot(self.update_pos, mean)\n",
    "\n",
    "        covariance = np.linalg.multi_dot((self.update_pos, covariance, self.update_pos.T))\n",
    "\n",
    "        return mean, covariance + innvoation_cov\n",
    "\n",
    "\n",
    "    def update(self, prediction, covariance, bounding_box):\n",
    "        # Step di correzione della predizione\n",
    "\n",
    "        projected_pred, projected_cov = self.project(prediction, covariance)\n",
    "\n",
    "        chol_factor, lower = scipy.linalg.cho_factor(\n",
    "            projected_cov, lower=True, check_finite=False)\n",
    "        kalman_gain = scipy.linalg.cho_solve((chol_factor, lower), np.dot(covariance, self.update_pos.T).T, \n",
    "                                             check_finite=False).T\n",
    "        innovation = bounding_box - projected_pred\n",
    "\n",
    "        new_pred = prediction + np.dot(innovation, kalman_gain.T)\n",
    "        new_covariance = covariance - np.linalg.multi_dot((\n",
    "            kalman_gain, projected_cov, kalman_gain.T))\n",
    "        \n",
    "        return new_pred, new_covariance\n",
    "    \n",
    "    def gating_distance(self, mean, covariance, measurements, only_position=False):\n",
    "        \"\"\" \n",
    "        Funzione che calcola il quadrato della distanza di Mahalanobis\n",
    "        \"\"\"\n",
    "        mean, covariance = self.project(mean, covariance)\n",
    "        if only_position:\n",
    "            mean, covariance = mean[:2], covariance[:2, :2]\n",
    "            measurements = measurements[:, :2]\n",
    "\n",
    "        cholesky_factor = np.linalg.cholesky(covariance)\n",
    "        d = measurements - mean\n",
    "        z = scipy.linalg.solve_triangular(cholesky_factor, d.T, lower=True, check_finite=False, overwrite_b=True)\n",
    "        squared_maha = np.sum(z*z, axis= 0)\n",
    "\n",
    "        return squared_maha\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intersection-over-uninion (IOU_matching)\n",
    "L'obiettivo di questa funzione è trovare il miglior match fra il bounding box che rileva l'oggetto e i vari 'track' candidati. Più in concreto si vuole calcolare l'area del bbox occupata dal candidato e ovviamente maggiore è tale area e migliore è il candidato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(bbox, candidate):\n",
    "    ''' \n",
    "    bbox = [x_min, y_min, width, height]\n",
    "    '''\n",
    "    # coordinate del bounding box\n",
    "    # NOTA: candidate potrebbe non essere in formato [x_min, y_min, width, height]\n",
    "\n",
    "    intersection = 0.0\n",
    "\n",
    "    # Coordinate del bbox\n",
    "    bx_min, by_min = bbox[0], bbox[1]\n",
    "    b_width, b_height = bbox[2], bbox[3]\n",
    "\n",
    "    # Coordinate del candidato\n",
    "    cx_min, cy_min = candidate[0], candidate[1]\n",
    "    c_width, c_height = candidate[2], candidate[3]\n",
    "\n",
    "    bbox_area = b_width * b_height\n",
    "    candidate_area = c_width * c_height\n",
    "\n",
    "    x_min = max(bx_min, cx_min)\n",
    "    y_min = max(by_min, cy_min)\n",
    "\n",
    "    x_max = min(bx_min+b_width, cx_min+c_width)\n",
    "    y_max = min(by_min+b_height, cy_min+c_height)\n",
    "\n",
    "    intersection = (x_max - x_min) * (y_max - y_min)\n",
    "\n",
    "    return intersection / (bbox_area + candidate_area - intersection)\n",
    "\n",
    "\n",
    "def iou_cost(tracks, detections):\n",
    "    \"\"\" \n",
    "    tracks: A list of tracks\n",
    "    detections: a list of detections\n",
    "\n",
    "    Returns cost_matrix\n",
    "    \"\"\"\n",
    "    INFINITY = 1e+5\n",
    "\n",
    "    rows = len(detections)\n",
    "    cols = len(tracks)\n",
    "\n",
    "    cost_matrix = np.zeros((rows, cols))\n",
    "\n",
    "    for j in range(cols):\n",
    "        if tracks[j].last_update > 1:\n",
    "            for i in range(rows):\n",
    "                cost_matrix[i, j] = INFINITY\n",
    "            continue\n",
    "\n",
    "        bbox = tracks[j]\n",
    "        max_score = 0.0\n",
    "        index = -1\n",
    "        for i in range(rows):\n",
    "            candidate = detections[i]\n",
    "            score = iou(bbox, candidate)\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                index = i \n",
    "        if index != -1:\n",
    "            cost_matrix[index, j] = 1. - max_score\n",
    "\n",
    "    return cost_matrix\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
