{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.utils.data as data \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import transforms, datasets, models\n",
    "from torchvision.io import read_image, read_video\n",
    "\n",
    "## Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Yolo\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparazione dei dati\n",
    "Il dataset originario era composto da 26067 immagini, mentre per motivi logistici in questo notebook è stato ridotto a 9014 immagini e 11 classi (invece delle 32 originarie). Per carciare le immagini usiamo la funzionalità `ImageLoader` presente in Pytorch che permette di estrarre tutte le immagini presenti in una cartella (e nelle sue sottocartelle) e assegna come lable a tali immagini il nome della cartella stessa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../archive\"\n",
    "\n",
    "class ImageFolderWithIndices(datasets.ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        image, label = super().__getitem__(index)\n",
    "\n",
    "        path = self.imgs[index][0]\n",
    "        filename = os.path.basename(path)\n",
    "\n",
    "        str_num = ''\n",
    "        for elem in filename:\n",
    "            if elem.isdigit():\n",
    "                str_num += elem\n",
    "\n",
    "        num = (int) (str_num)\n",
    "    \n",
    "        return image, label, num\n",
    "    \n",
    "dataset = ImageFolderWithIndices(root=dataset_path)\n",
    "#print(dataset.classes)\n",
    "#print(len(dataset))\n",
    "\n",
    "'''\n",
    "    Il dataset viene suddiviso nel modo seguente:\n",
    "        - 80% training_set\n",
    "        - 10% validation_set\n",
    "        - 10% test_set\n",
    "'''\n",
    "\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(dataset, [7212, 901, 901])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estrazione dei bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "bbox = [x_top_left, y_top_left, width, height, index], dove index è l'indice dell'immagine nella cartella in cui è salvata,\n",
    "questo serve per ricreare la corrispondenza fra immagine e lable\n",
    "'''\n",
    "\n",
    "bounding_boxes = []\n",
    "\n",
    "entries = sorted(os.listdir(dataset_path))\n",
    "for folder in entries:\n",
    "    sub_directory = os.path.join(dataset_path, folder)\n",
    "    tensor_list = []\n",
    "\n",
    "    for filename in os.listdir(sub_directory):\n",
    "        if filename.endswith(\"groundtruth_rect.txt\"):\n",
    "            path = os.path.join(sub_directory, filename)\n",
    "            f = open(path, \"r\")\n",
    "            lines = f.readlines()\n",
    "            len_file = len(lines)\n",
    "\n",
    "            for i in range(len_file):\n",
    "                line = lines[i].split()\n",
    "                floates = [float(x) for x in line]\n",
    "                index = (float)(i)\n",
    "                floates.append(index)\n",
    "                coordinates = torch.tensor(floates)\n",
    "                tensor_list.append(coordinates)\n",
    "        \n",
    "    bounding_boxes.append(tensor_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparazione del dataset per l'addestramento di YOLOv8 e ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"datasets/dataset\"\n",
    "resnet_dir = \"resnet_dir\"\n",
    "\n",
    "# creazione delle directory per Yolov8\n",
    "\n",
    "image_dirs = {\n",
    "    \"train\" : os.path.join(dataset_dir, \"images/train\"),\n",
    "    \"val\": os.path.join(dataset_dir, \"images/val\"),\n",
    "    \"test\": os.path.join(dataset_dir, \"images/test\")\n",
    "}\n",
    "\n",
    "label_dirs = {\n",
    "    \"train\": os.path.join(dataset_dir, \"labels/train\"),\n",
    "    \"val\": os.path.join(dataset_dir, \"labels/val\"),\n",
    "    \"test\": os.path.join(dataset_dir, \"labels/test\")\n",
    "}\n",
    "\n",
    "# Creazione delle directory per ResNet50\n",
    "\n",
    "dirs = {\n",
    "    \"train\" : os.path.join(resnet_dir, \"train\"),\n",
    "    \"val\" : os.path.join(resnet_dir, \"val\"),\n",
    "    \"test\" : os.path.join(resnet_dir, \"test\")\n",
    "}\n",
    "\n",
    "# Creazione delle cartelle\n",
    "\n",
    "for dir_path in image_dirs.values():\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "for dir_path in label_dirs.values():\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "for dir_path in dirs.values():\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# Salvataggio delle delle immagini e dei bounding boxes\n",
    "\n",
    "def save_images_and_labels(data, dataset_type):\n",
    "    image_dir = image_dirs[dataset_type]\n",
    "    label_dir = label_dirs[dataset_type]\n",
    "    cropped_image_dir = dirs[dataset_type]\n",
    "\n",
    "    for image, label, index in data:\n",
    "        img_save_path = os.path.join(image_dir, f\"{index}_{label}.jpg\")\n",
    "        image.save(img_save_path)\n",
    "\n",
    "        width = image.width\n",
    "        height = image.height\n",
    "\n",
    "        bounding_box = bounding_boxes[label][index-1]\n",
    "        bbox_width = bounding_box[2]\n",
    "        bbox_height = bounding_box[3]\n",
    "        \n",
    "        class_id = label\n",
    "\n",
    "        label_save_path = os.path.join(label_dir, f\"{index}_{label}.txt\")\n",
    "        with open(label_save_path, 'w') as f:\n",
    "\n",
    "            # normalizziamo il bbox\n",
    "\n",
    "            x_center = (bounding_box[0] + bbox_width/2) / width\n",
    "            y_center = (bounding_box[1] + bbox_height/2) / height\n",
    "            b_width = bbox_width / width\n",
    "            b_height = bbox_height / height\n",
    "\n",
    "            f.write(f\"{class_id} {x_center} {y_center} {b_width} {b_height}\\n\")\n",
    "\n",
    "        # Ritagliamo le immagini tenendoci solo la parte individuata dai bounding boxes\n",
    "\n",
    "        bbox_width = float(bbox_width)\n",
    "        bbox_height = float(bbox_height)\n",
    "        \n",
    "        x_min, y_min = round(float(bounding_box[0]), 1), round(float(bounding_box[1]), 1)\n",
    "        x_max = round(float(x_min + bbox_width), 1)\n",
    "        y_max = round(float(y_min + bbox_height), 1)\n",
    "\n",
    "        # Check che verifica che non si esca dai bordi dell'immagine o che non ci siano bounding boxes con \n",
    "        # altezza o spessore nullo\n",
    "        if x_min < 0 or y_min < 0 or x_max >= width or y_max >= height or x_max - x_min <= 0 or y_max - y_min <= 0:\n",
    "            continue\n",
    "        \n",
    "        cropped_image = image.crop((x_min, y_min, x_max, y_max))\n",
    "        cropped_save_path = os.path.join(cropped_image_dir, f\"{index}_{label}.jpg\")\n",
    "        cropped_image.save(cropped_save_path)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_images_and_labels(train_set, \"train\")\n",
    "save_images_and_labels(test_set, \"test\")\n",
    "save_images_and_labels(val_set, \"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yolov8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.train(data=\"file.yaml\", epochs=15, batch=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(\"datasets/dataset/images/test/10_7.jpg\", save=False, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracking con Yolov8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = model.track(source=\"Tracking_test/Ballena/Ballena.mp4\", conf=0.3, iou=0.6, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = model.track(source=\"Tracking_test/SeaDiver/SeaDiver.mp4\", conf=0.3, iou=0.5, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res3 = model.track(source=\"Tracking_test/Octopus2/Octopus2.mp4\", conf=0.3, iou=0.5, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Addestramento di ResNet50 per l'estrazione delle feature dai bounding boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nelle prime due celle di codice estraiamo le immagini per al fine di calcolare la loro media e deviazione standard per poterle normalizzare in modo che complessivamente la media e la deviazione standard su tutto il dataset siano approssimativamente 0 e 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "tmp_dataset = datasets.ImageFolder(root=resnet_dir, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([0.2522, 0.4443, 0.4805])\n",
      "std: tensor([0.1188, 0.1406, 0.1375])\n"
     ]
    }
   ],
   "source": [
    "# Andiamo a calcolare la media e la deviazione standard per \n",
    "# poter poi normalizzare le immagini\n",
    "\n",
    "dataset_size = len(tmp_dataset)\n",
    "\n",
    "DATA_MEAN = 0.0\n",
    "var = 0.0\n",
    "\n",
    "for i in range(dataset_size):\n",
    "    image, _ = tmp_dataset[i]\n",
    "    DATA_MEAN += image.mean(dim=(1, 2))\n",
    "    var += image.var(dim=(1, 2))\n",
    "    \n",
    "\n",
    "DATA_MEAN /= dataset_size\n",
    "DATA_STD = torch.sqrt(var/dataset_size)\n",
    "\n",
    "print(f\"Mean: {DATA_MEAN}\")\n",
    "print(f\"std: {DATA_STD}\")\n",
    "\n",
    "\"\"\" \n",
    "Mean: tensor([0.2522, 0.4443, 0.4805])\n",
    "std: tensor([0.1188, 0.1406, 0.1375])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nmodel.fc = nn.Identity()\\n\\nfeatures = model(image)\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.2522, 0.4443, 0.4805], [0.1188, 0.1406, 0.1375])\n",
    "])\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.2522, 0.4443, 0.4805], [0.1188, 0.1406, 0.1375])\n",
    "])\n",
    "\n",
    "train_dataset_path = \"resnet_dir/train1/train\"\n",
    "val_dataset_path = \"resnet_dir/val1/val\"\n",
    "test_dataset_path = \"resnet_dir/test1/test\"\n",
    "\n",
    "class CostumImageDataset(Dataset):\n",
    "    def __init__(self, image_folder, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.image_filenames = os.listdir(image_folder)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_folder, self.image_filenames[idx])\n",
    "        image = Image.open(img_path)\n",
    "        filename = self.image_filenames[idx].split('_')\n",
    "        category = filename[1].split('.')\n",
    "        label = int(category[0])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return (image, label)\n",
    "    \n",
    "\n",
    "train_dataset = CostumImageDataset(train_dataset_path, train_transform)\n",
    "val_dataset = CostumImageDataset(val_dataset_path, test_transform)\n",
    "test_dataset = CostumImageDataset(val_dataset_path, test_transform)\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "model.fc = nn.Identity()\n",
    "\n",
    "features = model(image)\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch mean tensor([-0.1059, -0.0629,  0.0056])\n",
      "Batch std tensor([1.3584, 1.3395, 1.6535])\n"
     ]
    }
   ],
   "source": [
    "imgs, _ = next(iter(train_loader))\n",
    "print(\"Batch mean\", imgs.mean(dim=[0,2,3]))\n",
    "print(\"Batch std\", imgs.std(dim=[0,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_resnet18(model, optimizer, data_loader, loss_module, num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    # Parallelize training accross multiple GPUs\n",
    "    # model = torch.nn.DataParallel(model)\n",
    "\n",
    "    loss = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, labels in data_loader:\n",
    "            # Questo passaggio è strettamente necessrio solo se si usa una gpu\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            preds = model(inputs)\n",
    "            preds = preds.squeeze(dim=1)\n",
    "\n",
    "            loss = loss_module(preds, labels)\n",
    "\n",
    "            # Prima di calcolare i gradienti ci assicuriamo che siano tutti zero\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Aggiornamento dei parametri\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model = models.resnet18(pretrained=True)\n",
    "num_ftrs = resnet_model.fc.in_features\n",
    "resnet_model.fc = nn.Linear(num_ftrs, 11)\n",
    "resent_model = resnet_model.to(device)\n",
    "\n",
    "loss_module = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(resnet_model.parameters(), lr=0.001, momentum=0.9)\n",
    "num_epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "train_resnet18(resnet_model, optimizer, train_loader, loss_module, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Salvataggio del modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = resnet_model.state_dict()\n",
    "torch.save(state_dict, \"my_resnet_model.pt\")\n",
    "\n",
    "\"\"\"\n",
    "# Specify a path\n",
    "PATH = \"state_dict_model.pt\"\n",
    "\n",
    "# Save\n",
    "torch.save(net.state_dict(), PATH)\n",
    "\n",
    "# Load\n",
    "model = Net()\n",
    "model.load_state_dict(torch.load(PATH, weights_only=True))\n",
    "model.eval()\n",
    "\"\"\"\n",
    "\n",
    "# Salva l'intero modello\n",
    "torch.save(resnet_model, 'entire_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valuatazione del modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data, loss_module):\n",
    "\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data:\n",
    "            \n",
    "            #inputs = inputs.to(device)\n",
    "            #labels = labels.to(device)\n",
    "\n",
    "            preds = model(inputs)\n",
    "            loss = loss_module(preds, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = preds.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuarcy:  100.0\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18()\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 11)\n",
    "model.load_state_dict(torch.load(\"my_resnet_model.pt\", map_location='cpu'))\n",
    "model.eval()\n",
    "loss_module = nn.CrossEntropyLoss()\n",
    "\n",
    "acc = evaluate_model(model, test_loader, loss_module)\n",
    "\n",
    "print(\"accuarcy: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepSORT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
